{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "- Use the FLD data field tag to break string from field\n",
    "- Remove long digit by xdig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import html\n",
    "import pandas as pd\n",
    "import json\n",
    "from requests import Session\n",
    "MATIFY_API_EP = 'http://matify.net:8000/'\n",
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "NUM = 'xdig'  # Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=Path('data/matify/')\n",
    "PATH.mkdir(exist_ok=True)\n",
    "PROD_FILE = PATH/'text'/'matify_product.csv'\n",
    "CLAS_PATH = PATH/'classifier'\n",
    "CLAS_PATH.mkdir(exist_ok=True)\n",
    "LM_PATH=PATH/'language_model'\n",
    "LM_PATH.mkdir(exist_ok=True)\n",
    "W2V_PATH=PATH/'w2v'\n",
    "W2V_PATH.mkdir(exist_ok=True)\n",
    "W2V_FILE=W2V_PATH/'fasttext.no.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from niklib.w2v_model import Word2VecModel\n",
    "w2v = Word2VecModel.from_pickle(W2V_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SubCategories ():\n",
    "    session = Session()\n",
    "    categoryResponse = session.get(MATIFY_API_EP + 'listCategories/', \n",
    "                                   headers={'Accept':'application/json'})\n",
    "    assert (int(categoryResponse.status_code) == 200), \\\n",
    "            \"Error when requesting all categories. Response text: \" + categoryResponse.text\n",
    "    categories = json.loads(categoryResponse.text)\n",
    "    for category in categories:\n",
    "        for subCategory in category['sub_categories']:\n",
    "            yield (subCategory['id'], subCategory['name'])\n",
    "\n",
    "id2subcat = {id: subcat for id, subcat in get_SubCategories()}\n",
    "id2subcat[142] = 'NotFood'\n",
    "# id2subcat[-1] = 'NoLabel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv(PROD_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"labels\"] = df_full[\"category_id\"]\n",
    "df[\"name\"] = df_full[\"name\"] \n",
    "df[\"description\"] = df_full[\"description\"]\n",
    "df = df[~pd.isna(df[\"name\"])] # remove no text\n",
    "# df = df[~pd.isna(df[\"description\"])] # remove no text\n",
    "df[\"labels\"].fillna(-1, inplace=True) # -1 for no label\n",
    "df = df[df['labels'].isin(list(id2subcat.keys()) + [-1])] # only keep valid label, including no label\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "np.random.seed(42)\n",
    "idx = np.random.permutation(len(df))\n",
    "df = df.iloc[idx]\n",
    "df_w_labels = df[df[\"labels\"]!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split label data\n",
    "df_trn, df_val = sklearn.model_selection.train_test_split(df_w_labels, test_size=0.2, stratify=df_w_labels[\"labels\"])\n",
    "df_trn.to_csv(CLAS_PATH/'train.csv', header=False, index=False)\n",
    "df_val.to_csv(CLAS_PATH/'test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.DataFrame(list(id2subcat.items()), columns=['id', 'subcat'])\n",
    "classes.to_csv(CLAS_PATH/'classes.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn, df_val = sklearn.model_selection.train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_trn), len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn.to_csv(LM_PATH/'train.csv', header=False, index=False)\n",
    "df_val.to_csv(LM_PATH/'test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize=24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_popularity(word, w2v):\n",
    "    unknown_pop_score = w2v.n_vocabs\n",
    "    if w2v.word2idx(word) > 0:\n",
    "        return w2v.word2idx(word), [word]\n",
    "        \n",
    "    if (word.replace('.','',1).replace(',','',1).isdigit())  : #all numbers replaced by xdig. except single digit\n",
    "        # return unknown_pop_score, [word] # Don't care about digit\n",
    "        return unknown_pop_score, [f'{NUM}']\n",
    "\n",
    "    if len(word)<=5:\n",
    "        return unknown_pop_score, [word]\n",
    "\n",
    "\n",
    "    best_pop_score = unknown_pop_score\n",
    "    best_split = None\n",
    "    best_nsplit = 3 # Max split to split\n",
    "    for i_cut in range(len(word)-2, 1, -1):\n",
    "        prefix, core = word[:i_cut], word[i_cut:]\n",
    "\n",
    "        if w2v.word2idx(core) < 0:\n",
    "            continue\n",
    "        core_score = w2v.word2idx(core)\n",
    "\n",
    "        if w2v.word2idx(prefix) < 0:\n",
    "            prefix_score, prefix = split_by_popularity(prefix, w2v)\n",
    "        else:\n",
    "            prefix_score, prefix = w2v.word2idx(prefix), [prefix]                \n",
    "        if (prefix_score >= unknown_pop_score): # Don't split if all splitted words are good\n",
    "            continue\n",
    "        pop_score = prefix_score + core_score\n",
    "        words = prefix; words.append(core)\n",
    "\n",
    "        if len(words) > best_nsplit: continue # Don't split more than best_nsplit\n",
    "        if (len(words) < best_nsplit) or (pop_score < best_pop_score):\n",
    "            #print(prefix, prefix_score, core_score, pop_score)\n",
    "            best_split = words\n",
    "            best_nsplit= len(words)\n",
    "            best_pop_score = pop_score\n",
    "\n",
    "    if best_split is None:\n",
    "        return (unknown_pop_score, [word])\n",
    "    else:\n",
    "        return (best_pop_score, best_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re1 = re.compile(r'  +')\n",
    "\n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(df, w2v, n_lbls=1):\n",
    "    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD}1 ' + df[n_lbls].astype(str)\n",
    "    for i in range(n_lbls+1, len(df.columns)): texts += f' {FLD}{i+1-n_lbls} ' + df[i].astype(str)\n",
    "    texts = texts.apply(fixup).values.astype(str)\n",
    "\n",
    "    alltoks = Tokenizer().proc_all_mp(partition_by_cores(texts)) # Note: apply our split_by_popularity algorithm\n",
    "    alltoks = [[split_by_popularity(tok, w2v)[1] for tok in toks] for toks in alltoks]\n",
    "    alltoks = [[newtok for newtoks in toks for newtok in newtoks] for toks in alltoks] # flatten list\n",
    "    return alltoks, list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(df, w2v, n_lbls): # Used when df is chunked\n",
    "    tok, labels = [], []\n",
    "    for i, r in enumerate(df):\n",
    "        print(i)\n",
    "        tok_, labels_ = get_texts(r, w2v, n_lbls)\n",
    "        tok += tok_\n",
    "        labels += labels_\n",
    "    return tok, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv(LM_PATH/'train.csv', header=None, chunksize=chunksize)\n",
    "df_val = pd.read_csv(LM_PATH/'test.csv', header=None, chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn, trn_labels = get_all(df_trn, w2v, 1)\n",
    "tok_val, val_labels = get_all(df_val, w2v, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tok_trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(LM_PATH/'tmp').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(LM_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
    "np.save(LM_PATH/'tmp'/'tok_val.npy', tok_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq = Counter(p for o in tok_trn for p in o)\n",
    "freq.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 150000\n",
    "min_freq = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_lm = np.array([[stoi[o] for o in p] for p in tok_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(LM_PATH/'tmp'/'trn_ids.npy', trn_lm)\n",
    "np.save(LM_PATH/'tmp'/'val_ids.npy', val_lm)\n",
    "pickle.dump(itos, open(LM_PATH/'tmp'/'itos.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lm = np.load(LM_PATH/'tmp'/'trn_ids.npy')\n",
    "val_lm = np.load(LM_PATH/'tmp'/'val_ids.npy')\n",
    "itos = pickle.load(open(LM_PATH/'tmp'/'itos.pkl', 'rb'))\n",
    "stoi = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge itos and stoi with Fasttext vocab and embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz = 300\n",
    "# Get itos2 and stoi2 of the W2V dict\n",
    "itos2 = w2v.idx2word\n",
    "stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)})\n",
    "\n",
    "# 1. Build vocab and emb for the first vocabs\n",
    "row_m = w2v.idx2emb.mean(0)\n",
    "new_w = np.zeros((max_vocab, em_sz), dtype=np.float32)\n",
    "for i, w in enumerate(itos):\n",
    "    r = stoi2[w]\n",
    "    new_w[i] = w2v.idx2emb[r] if r>=0 else row_m\n",
    "    \n",
    "# 2. Get the rest from the second vocab\n",
    "idx = len(itos)\n",
    "for i, w in enumerate(itos2):\n",
    "    if stoi[w] < 0:\n",
    "        itos.append(w)\n",
    "        stoi[w] = idx\n",
    "        new_w[idx] = w2v.idx2emb[i]\n",
    "        idx += 1\n",
    "    if idx >= max_vocab: break\n",
    "\n",
    "pickle.dump(itos, open(LM_PATH/'tmp'/'itos_full.pkl', 'wb'))\n",
    "itos = pickle.load(open(LM_PATH/'tmp'/'itos_full.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz, nh, nl = 300,600,1\n",
    "vs=len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-7\n",
    "bptt=50 # instead of 70 bptt in fasttext\n",
    "bs=32\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\n",
    "md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner= md.get_model(opt_fn, em_sz, nh, nl, \n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
    "learner.metrics = [accuracy]\n",
    "learner.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = learner.model.state_dict()\n",
    "wgts['0.encoder.weight'] = T(new_w)\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\n",
    "wgts['1.decoder.weight'] = T(np.copy(new_w))\n",
    "learner.model.load_state_dict(wgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "lrs = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learner.fit(lrs/2, 2, wds=wd, use_clr=(32,2), cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('lm_last_ft')\n",
    "learner.load('lm_last_ft')\n",
    "learner.unfreeze()\n",
    "learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learner.fit(0.01/2, 1, wds=wd, use_clr=(20,10), cycle_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(0.01/4, 3, wds=wd, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('lm1')\n",
    "learner.save_encoder('lm1_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv(CLAS_PATH/'train.csv', header=None, chunksize=24000)\n",
    "df_val = pd.read_csv(CLAS_PATH/'test.csv', header=None, chunksize=24000)\n",
    "tok_trn, trn_labels = get_all(df_trn, w2v, 1)\n",
    "tok_val, val_labels = get_all(df_val, w2v, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CLAS_PATH/'tmp').mkdir(exist_ok=True)\n",
    "\n",
    "np.save(CLAS_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
    "np.save(CLAS_PATH/'tmp'/'tok_val.npy', tok_val)\n",
    "\n",
    "np.save(CLAS_PATH/'tmp'/'trn_labels.npy', trn_labels)\n",
    "np.save(CLAS_PATH/'tmp'/'val_labels.npy', val_labels)\n",
    "id2subcatid = list(np.unique(trn_labels))\n",
    "pickle.dump(id2subcatid, open(CLAS_PATH/'id2subcatid.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn = np.load(CLAS_PATH/'tmp'/'tok_trn.npy')\n",
    "tok_val = np.load(CLAS_PATH/'tmp'/'tok_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = pickle.load((LM_PATH/'tmp'/'itos_full.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_clas = np.array([[stoi[o] for o in p] for p in tok_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'trn_labels.npy'))\n",
    "val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels.npy'))\n",
    "id2subcatid = pickle.load(open(CLAS_PATH/'id2subcatid.pkl', 'rb'))\n",
    "subcatid2id = {v:k for k, v in enumerate(id2subcatid)}\n",
    "trn_labels = np.array([ subcatid2id[label] for label in trn_labels ])\n",
    "val_labels = np.array([ subcatid2id[label] for label in val_labels ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt,em_sz,nh,nl = 50,300,600,1\n",
    "c = len(id2subcatid)\n",
    "vs = len(itos)\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "bs = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = TextDataset(trn_clas, trn_labels)\n",
    "val_ds = TextDataset(val_clas, val_labels)\n",
    "trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
    "val_samp = SortSampler(val_clas, key=lambda x: len(val_clas[x]))\n",
    "trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "md = ModelData(PATH, trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dps = np.array([0.4,0.5,0.05,0.3,0.4])# * 0.5\n",
    "# dps = np.array([0.2]*5)\n",
    "m = get_rnn_classifer(bptt, 20*bptt, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "          layers=[em_sz*3, 1000, c], drops=[dps[4], 0.1],\n",
    "          dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "\n",
    "learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learn.clip=25.\n",
    "learn.metrics = [accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=3e-3\n",
    "lrm = 2.6\n",
    "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "#lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])\n",
    "#lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-3])\n",
    "lrs = 1e-3\n",
    "wd = 1e-7\n",
    "# wd = 0\n",
    "learn.load_encoder('lm1_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(learn.models.get_layer_groups())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-1)\n",
    "learn.lr_find(lrs/1000)\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('clas_0')\n",
    "learn.load('clas_0')\n",
    "learn.freeze_to(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs, 10, wds=wd, cycle_len=1, use_clr=(8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('clas_1')\n",
    "learn.load('clas_1')\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(lrs/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lrs, 2, wds=wd, cycle_len=25, use_clr=(32,10), cycle_mult=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('clas_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE_GPU = False\n",
    "itos = pickle.load((LM_PATH/'tmp'/'itos_full.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "classes = pd.read_csv(CLAS_PATH/'classes.csv', header=None)\n",
    "id2subcatid = pickle.load(open(CLAS_PATH/'id2subcatid.pkl', 'rb'))\n",
    "\n",
    "bptt,em_sz,nh,nl = 40,300,600,1\n",
    "c = len(id2subcatid)\n",
    "vs = len(itos)\n",
    "bs = 48\n",
    "\n",
    "m = get_rnn_classifer(bptt, 20*bptt, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "          layers=[em_sz*3, 1000, c], drops=[dps[4], 0.1],\n",
    "          dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
    "load_model(m, PATH/'models'/'clas_final.h5')\n",
    "m = to_gpu(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(data=[[0, 'Street food grillfakkel Grill perfekt. 200 g. Pr kg 249,50'],\n",
    "                   [0, 'Hatting pølsebrød'],\n",
    "                   [0, 'Street food baby back ribs Grill perfekt. 800 g. Pr kg 186,25'],\n",
    "                   [0, 'Santa maria bbq sauce 3 varianter. 420-470 g. Pr kg fra 63,62'],\n",
    "                   [0, 'Grill perfekt kyllingfilet Tex. BBQ. 400 g. Pr kg 174,75'],\n",
    "                   [0, 'Street food kyllingspyd Grill perfekt. Thai eller BBQ. 180 g. Pr kg 221,67'],\n",
    "                   [0, 'Grill perfekt flintstek Krydret. Pr kg'],\n",
    "                   [0, 'Coop gresk landbrød Butikkstekt. 560 g. Pr kg 44,46'],\n",
    "                   [0, 'Coop fiber & frøbrød 650 g. Pr kg 30,62']]) # always put labels first and data field later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_test, test_labels = get_texts(df_test, w2v, 1)\n",
    "test_clas = np.array([[stoi[o] for o in p] for p in tok_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tok_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = TextDataset(test_clas, test_labels)\n",
    "test_dl = DataLoader(test_ds, batch_size=100, transpose=True, num_workers=1, pad_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.eval()\n",
    "for x, y in test_dl:\n",
    "    output = to_np(m(V(x)))[0] # Only get the final prediction, no hidden states\n",
    "    preds = np.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [id2subcatid[id] for id in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcatid2name ={item[0]:item[1] for idx, item in classes.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_subcat = [subcatid2name[id] for id in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_subcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
